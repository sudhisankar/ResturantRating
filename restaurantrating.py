# -*- coding: utf-8 -*-
"""RestaurantRating.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1fDwxSz5S3PwLpKXgj08FlBRnKo9FjCys
"""

import pandas as pd
import numpy as np
import matplotlib.pyplot as plt
import seaborn as sns
from sklearn.model_selection import train_test_split
from sklearn.preprocessing import LabelEncoder, StandardScaler
from sklearn.linear_model import LinearRegression
from sklearn.tree import DecisionTreeRegressor
from sklearn.metrics import mean_squared_error, r2_score

file_path = "/content/Dataset .csv"
df = pd.read_csv(file_path)
df.head()  # Display the first few rows

print(df.isnull().sum())  # Check missing values

# Fill missing numerical values with the median
for col in df.select_dtypes(include=['number']).columns:
    df[col].fillna(df[col].median(), inplace=True)

# Fill missing categorical values with the mode
for col in df.select_dtypes(include=['object']).columns:
    df[col].fillna(df[col].mode()[0], inplace=True)

label_encoders = {}
for col in df.select_dtypes(include=['object']).columns:
    le = LabelEncoder()
    df[col] = le.fit_transform(df[col])
    label_encoders[col] = le  # Store encoders for future use

print(df.columns)

target_col = "Aggregate rating"  # Use this exact name from df.columns
  # Use the correct name from print(df.columns)

# Ensure no leading/trailing spaces
df.columns = df.columns.str.strip()

X = df.drop(columns=[target_col])
y = df[target_col]

print(df.head())  # Check the first few rows
print(df.columns) # Verify column names

from sklearn.model_selection import train_test_split

df.columns = df.columns.str.strip()  # Remove extra spaces if any

X = df.drop(columns=[target_col])  # Drop target column
y = df[target_col]  # Target variable

X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)

print("Training set size:", X_train.shape, y_train.shape)
print("Testing set size:", X_test.shape, y_test.shape)

from sklearn.linear_model import LinearRegression

lr_model = LinearRegression()
lr_model.fit(X_train, y_train)

y_pred_lr = lr_model.predict(X_test)

from sklearn.metrics import mean_squared_error, r2_score
print("Linear Regression MSE:", mean_squared_error(y_test, y_pred_lr))
print("Linear Regression R2:", r2_score(y_test, y_pred_lr))

dt_model = DecisionTreeRegressor()
dt_model.fit(X_train, y_train)

# Predictions
y_pred_dt = dt_model.predict(X_test)

# Evaluate
mse_dt = mean_squared_error(y_test, y_pred_dt)
r2_dt = r2_score(y_test, y_pred_dt)
print(f"Decision Tree Regression - MSE: {mse_dt}, R-squared: {r2_dt}")

feature_importances = pd.Series(dt_model.feature_importances_, index=X.columns)
feature_importances.sort_values(ascending=False).plot(kind="bar", figsize=(10,5))
plt.title("Feature Importance in Decision Tree Model")
plt.show()

from sklearn.metrics import mean_squared_error, r2_score

# Train Linear Regression Model
lr_model = LinearRegression()
lr_model.fit(X_train, y_train)

# Predictions
y_pred_lr = lr_model.predict(X_test)

# Compute Metrics
mse_lr = mean_squared_error(y_test, y_pred_lr)
r2_lr = r2_score(y_test, y_pred_lr)

from sklearn.tree import DecisionTreeRegressor

# Train Decision Tree Model
dt_model = DecisionTreeRegressor()
dt_model.fit(X_train, y_train)

# Predictions
y_pred_dt = dt_model.predict(X_test)

# Compute Metrics
mse_dt = mean_squared_error(y_test, y_pred_dt)
r2_dt = r2_score(y_test, y_pred_dt)

print("Model Performance Comparison:")
print(f"Linear Regression -> MSE: {mse_lr:.4f}, R2: {r2_lr:.4f}")
print(f"Decision Tree Regression -> MSE: {mse_dt:.4f}, R2: {r2_dt:.4f}")

from sklearn.ensemble import RandomForestRegressor

rf_model = RandomForestRegressor()
rf_model.fit(X_train, y_train)

y_pred_rf = rf_model.predict(X_test)

mse_rf = mean_squared_error(y_test, y_pred_rf)
r2_rf = r2_score(y_test, y_pred_rf)

print(f"Random Forest -> MSE: {mse_rf:.4f}, R2: {r2_rf:.4f}")

import matplotlib.pyplot as plt
import seaborn as sns

# Get feature importance
importances = rf_model.feature_importances_
feature_names = X.columns

# Sort and plot
sorted_indices = importances.argsort()[::-1]
plt.figure(figsize=(10, 5))
sns.barplot(x=importances[sorted_indices], y=[feature_names[i] for i in sorted_indices])
plt.xlabel("Feature Importance")
plt.ylabel("Features")
plt.title("Random Forest Feature Importance")
plt.show()

train_pred_rf = rf_model.predict(X_train)
mse_train_rf = mean_squared_error(y_train, train_pred_rf)
r2_train_rf = r2_score(y_train, train_pred_rf)

print(f"Random Forest (Train) -> MSE: {mse_train_rf:.4f}, R2: {r2_train_rf:.4f}")
print(f"Random Forest (Test)  -> MSE: {mse_rf:.4f}, R2: {r2_rf:.4f}")
from sklearn.model_selection import train_test_split
from sklearn.linear_model import LinearRegression
import matplotlib.pyplot as plt
import numpy as np

# Define target variable
target_col = "Aggregate rating"  # Ensure this matches your dataset

# Drop non-numeric or unnecessary columns
X = df.drop(columns=["Restaurant ID", "Restaurant Name", "Address", "Locality",
                     "Locality Verbose", "Cuisines", "Currency", "Aggregate rating"])

y = df[target_col]  # Target variable

# Train-test split
X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)

# Train the Linear Regression model
lr_model = LinearRegression()
lr_model.fit(X_train, y_train)

y_pred_lr = lr_model.predict(X_test)

plt.figure(figsize=(8, 5))

# Choose one feature (e.g., Votes)
feature = "Votes"

plt.scatter(X_test[feature], y_test, color="blue", label="Actual Ratings")  # True values
plt.scatter(X_test[feature], y_pred_lr, color="red", label="Predicted Ratings")  # Predicted values

plt.xlabel(feature)
plt.ylabel("Aggregate Rating")
plt.title(f"Linear Regression Predictions for {feature}")
plt.legend()
plt.show()

plt.figure(figsize=(8, 5))

# Choose one feature (e.g., "Votes")
feature = "Votes"
X_feature = X_test[feature].values.reshape(-1, 1)

# Train a simple Linear Regression model using only one feature
lr_single = LinearRegression()
lr_single.fit(X_feature, y_test)

# Generate predictions
y_pred_feature = lr_single.predict(X_feature)

# Plot data points and regression line
plt.scatter(X_feature, y_test, color="blue", label="Actual Ratings")
plt.plot(X_feature, y_pred_feature, color="red", linewidth=2, label="Regression Line")

plt.xlabel(feature)
plt.ylabel("Aggregate Rating")
plt.title(f"Linear Regression Line for {feature}")
plt.legend()
plt.show()

coefficients = lr_model.coef_
features = X.columns

plt.figure(figsize=(10, 6))
plt.barh(features, coefficients, color="blue")
plt.xlabel("Coefficient Value")
plt.ylabel("Features")
plt.title("Feature Importance in Linear Regression")
plt.show()